---
title: "BlogPost"
author: "Felipe Sodré Mendes Barros"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
### Antes de empezar:
Algunas referencias:  

- https://rpubs.com/jboscomendoza/analisis_sentimientos_lexico_afinn  
- https://shiring.github.io/text_analysis/2017/06/28/twitter_post   
- https://uc-r.github.io/hc_clustering   
- https://analyzecore.com/tag/twitter-sentiment-analysis/  

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
# creando un tema ggplot
```

### CumbreG20


```{r}
library(rtweet)
library(tidyverse)
library(lubridate)
# text mining library
library(tidytext)
library(wordcloud2)
library(data.table)
library(ggplot2)
#library(cluster)    # clustering algorithms
#library(factoextra) # clustering visualization
#library(tm)

tema_graf <-
  theme_minimal() +
  theme(text = element_text(family = "serif"),
        panel.grid.minor = element_blank(),
        strip.background = element_rect(fill = "#EBEBEB", colour = NA),
        #legend.position = "none",
        legend.box.background = element_rect(fill = "#EBEBEB", colour = NA))
# cargando lexico
afinn <- read.csv("lexico_afinn.en.es.csv", stringsAsFactors = F, fileEncoding = "latin1") %>% 
  tbl_df()

# Identificando stopword
stopword.es <- stopwordslangs %>% filter(lang == "es" & p >= 0.80) %>% select(word)
# inglés
stopword.en <- stopwordslangs %>% filter(lang == "en" & p >= 0.80) %>% select(word)
```
#### Configurando y descargando datos
```{r, eval=FALSE, echo=FALSE}
consumer_key <- "H4ljPlljHfQacfDhbDXCpCd37"
consumer_secret <- "CferzYTaf822R6U8lSWt995Ek681kXBnX5Gt6ysWF45TzLrYMX"
access_token <- "3323332149-x8GNuo7ziFaRgJiPnBOOH2mRLdUJuZy4Wnbe2bs"
access_secret <- "6SRUNmVoyK5cGkgYVOjblcAEFL7TyzNe1TgjaL3fy8uN3"
create_token(
  app = "ciencia de datos con R",
  consumer_key,
  consumer_secret,
  access_token,
  access_secret)

## search for 18000 tweets using the rstats hashtag
CumbreG20 <- search_tweets( "#CumbreG20", n = 18000, include_rts = FALSE, type = "recent", retryonratelimit = TRUE)
#write_as_csv(CumbreG20, "./datos/CumbreG20_7.csv")

G20Argentina <- search_tweets( "#G20Argentina", n = 18000, include_rts = FALSE, type = "recent", retryonratelimit = TRUE)
#write_as_csv(G20Argentina, "./datos/G20Argentina_7.csv")

G20Summit <- search_tweets( "#G20Summit", n = 18000, include_rts = FALSE, type = "recent", retryonratelimit = TRUE)
#write_as_csv(G20Summit, "./datos/G20Summit_7.csv")

g20org <- search_tweets( "@g20org", n = 18000, include_rts = FALSE, type = "recent", retryonratelimit = TRUE)
#write_as_csv(G20Summit, "./datos/g20org_7.csv")
```

### Cargando los datos  

** basta ejecutar la ultima linea para cargar todos los datos.**  

```{r, eval=FALSE, echo=FALSE}
# Entendiendo los datos
datosList <- read_twitter_csv(list.files("./datos", full.names = TRUE)[1], unflatten = TRUE)

colnames(datosList)

datosList[1,"account_lang"]
"account_lang"
datosList[1,"verified"]
"verified"
datosList[1,"favourites_count"]
"favourites_count" 
datosList[1,"geo_coords"]
"geo_coords"
datosList[1,"quoted_location"]
datosList[1,"place_name"]

datosList[1,"quoted_screen_name"]

datosList[1,"lang"][[1]]
"lang" 
datosList[1,"mentions_screen_name"][[1]]
"mentions_screen_name"
datosList[1,"hashtags"][[1]]
"hashtags"
datosList[1,"text"]
datosList[1,"screen_name"]
datosList[1,"mentions_screen_name"][[1]]


# Necesitamos buscar una forma de leer todos los csv sin hacer falta hacelo a cada uno

datosList <- list.files("./datos", full.names = TRUE)
tweetts <- factor()
for (a in datosList){
  # a = datosList[2]
  tweet <- read_twitter_csv(a, unflatten = TRUE)
  tweets <- bind_rows(tweets, tweet)
}
# write_as_csv(tweets, "./datos/TodosDatosG20.csv")
```

```{r}
tweets <- read_twitter_csv("./datos/TodosDatosG20.csv")
tweets <- tweets %>% 
  distinct(status_id, .keep_all = TRUE)
```
  
### Preguntas:  

Por ahora se me ocurrió algunas preguntas como:  
  
- Cuales nombres fueron mencionados en conjunto con otros (nombre asociados entre si);  
- Que "presidente" es más influyente?
  - Más mencionado?
  - Más favoritado?
  - Sentimentos relacionados a los distintos presidentes;
  
#### Cuales lenguas publicaron mas?  

```{r}
tweets %>% 
  count(lang) %>%
  droplevels() %>%
  top_n(2) %>% 
  ggplot(aes(x = reorder(lang, desc(n)), y = n)) +
    geom_bar(stat = "identity", alpha = 0.8) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    labs(x = "Lengua",
         y = "Cantidad de tweets") + 
  tema_graf
```

# Cual fué la cantidad de likes?  

```{r}
tweets %>% 
  filter(lang %in% c("es", "en")) %>% 
  group_by(lang) %>% 
  ggplot(aes(x = log2(favourites_count), colour = lang, group = lang)) +
    geom_density(alpha = 0.8) +
    labs(x = "Cantidad de likes",
         y = "densidad") + 
  tema_graf
```
#### Hashtags más usadas:
```{r, eval=FALSE}
tweets %>% 
  unnest_tokens(input = "hashtags", output = "Hashtags") %>% 
  select(Hashtags) %>% 
  count(Hashtags,sort = TRUE) %>% 
  top_n(50) %>% 
  wordcloud2(size = .5, maxRotation = 0, shuffle = FALSE)
```

#### Cuales lideres fueron mas mencionados?  

```{r}
lideres <- c("abeshinzo",
             "mauriciomacri",
             "AngelaMerkeICDU",
             "realdonaldtrump",
             "theresa_may",
             "rt_erdogan",
             "sanchezcastejon",
             "cyrilramaphosa",
             "leehsienloong",
             "macky_sall",
             "kingsalman",
             "MohammedbinSalman",
             "paulkagame",
             "kremlinrussia_e",
             "_moonjae_in",
             "minpres",
             "epn",
             "andrewholnessjm",
             "GiuseppeConteIT",
             "jokowi",
             "narendramodi",
             "EmmanuelMacron",
             "eucopresident",
             "JunckerEU",
             "xjJinping",
             "sebastianpinera",
             "JustinTrudeau",
             "MichelTemer",
             "ScottMorrisonMP",
             "putin")
lideres <- lideres %>% tolower()
```
```{r}
tweets %>% 
  unnest_tokens(input = "mentions_screen_name", output = "Mentions") %>% 
  select(Mentions) %>% 
  filter(!is.na(Mentions), Mentions %in% lideres) %>% 
  count(Mentions, sort = TRUE) %>% 
  top_n(3) %>% 
  ggplot( aes(reorder(Mentions, -n), n)) + 
  geom_col() + 
  labs(x = "Cantidad de menciones",
         y = "Lideres") + 
  tema_graf
```
#### Cual lider mas mencionado con hashtag?
```{r}
tweets %>% 
  unnest_tokens(input = "hashtags", output = "Hashtags") %>% 
  select(Hashtags) %>% 
  filter(!is.na(Hashtags), Hashtags %in% lideres) %>% 
  count(Hashtags, sort = TRUE) %>% 
  top_n(3) %>% 
  ggplot( aes(reorder(Hashtags, -n), n)) + 
  geom_col() + 
  labs(x = "Cantidad de menciones por #",
         y = "Lideres") + 
  tema_graf
```
#### Juntando hashtags and Mentions

```{r, eval=TRUE}
MergedMentions <- tweets %>% 
  unnest_tokens(input = "mentions_screen_name", output = "Mentions") %>% 
  filter(!is.na(Mentions), Mentions %in% lideres) %>% 
  unnest_tokens(input = "hashtags", output = "Hashtags") %>% 
  gather("origin", "MentionsMerged", Mentions, Hashtags) %>% 
  filter(!is.na(MentionsMerged), MentionsMerged %in% lideres)

MergedMentions %>% 
  count(MentionsMerged, sort = TRUE) %>% 
  top_n(3) %>% 
  ggplot( aes(reorder(MentionsMerged, -n), n)) + 
  geom_col() +
  labs(x = "Cantidad de likes",
         y = "densidad") + 
  tema_graf
```

### Wordcloud
```{r}
afinnALL <- afinn %>% gather("Categoria", "Palabra", Palabra, Word) %>% 
  select(Puntuacion, Palabra)
# Separando ES, EN y filtrando dias de cumbre
tw.en <- 
  tweets %>%
  mutate(Dia = day(created_at),
         Mes = month(created_at),
         Hora = hour(created_at),
         text = tolower(text)) %>% 
  filter((Dia == 30 | Dia == 1) & lang == "en") %>%   # filtrando días de la cumbre
    unnest_tokens(input = "text", output = "Word") %>% # separa cada palabra del texto
  inner_join(afinn, ., by = "Word") %>% # join con lexico es
  mutate(Tipo = ifelse(Puntuacion > 0, "Positiva", "Negativa")) %>% 
  filter(! Palabra %in% stopword.en$word )

tw.es <- 
  tweets %>%
  mutate(Dia = day(created_at),
         Mes = month(created_at),
         Hora = hour(created_at),
         text = tolower(text)) %>% 
  filter((Dia == 30 | Dia == 1) & lang == "es") %>%   # filtrando días de la cumbre
  unnest_tokens(input = "text", output = "Palabra") %>% # separa cada palabra del texto
  inner_join(afinnALL, ., by = "Palabra") %>% # join con lexico es
  mutate(Tipo = ifelse(Puntuacion > 0, "Positiva", "Negativa")) %>% filter(! Palabra %in% stopword.es$word )

all <- MergedMentions %>%
  mutate(Dia = day(created_at),
         Mes = month(created_at),
         Hora = hour(created_at),
         text = tolower(text)) %>% 
  filter(Dia == 30 | Dia == 1) %>%   # filtrando días de la cumbre
  unnest_tokens(input = "text", output = "Palabra") %>%
  inner_join(afinnALL, ., by = "Palabra") %>% # join con lexico
  mutate(Tipo = ifelse(Puntuacion > 0, "Positiva", "Negativa")) %>% 
  filter((! Palabra %in% stopword.en$word) | (! Palabra %in% stopword.es$word) )
```
```{r}
all %>% 
  group_by(Palabra) %>% 
  count(sort = T) %>% 
  top_n(50) %>% 
  wordcloud2(size = 0.5, maxRotation = 0, shuffle = FALSE)
```
```{r}
tw.es %>% 
  group_by(Palabra) %>% 
  count(sort = T) %>% 
  top_n(50) %>% 
  wordcloud2(size = 0.5, maxRotation = 0, shuffle = FALSE)

#tw.en %>% 
#  group_by(Word) %>% 
#  count(sort = T) %>% 
#  wordcloud2(size = 0.5, maxRotation = 0, shuffle = FALSE)
```


#### Con que tipo de sentimiento cada representante?
```{r}
all %>% 
  group_by(MentionsMerged, status_id) %>% 
  filter((!is.na(MentionsMerged)), MentionsMerged %in% lideres) %>%
  summarise(Promedio = mean(Puntuacion)) %>% 
  summarise(PromedioLider = mean(Promedio)) %>% 
  mutate(Tipo = ifelse(PromedioLider >= 0, "Positiva", "Negativa")) %>% 
  filter(Tipo == "Negativa") %>% 
  ggplot( aes(reorder(MentionsMerged, -PromedioLider), PromedioLider, fill = Tipo)) +
  geom_col() +
  coord_flip() +
  labs(x = "Lider",
         y = "") + 
  tema_graf
```

#### Que palabras estan asociadas a kingsalman y mohammedbinsalman?
```{r}
all %>% 
  filter((MentionsMerged == "kingsalman") | (MentionsMerged == "mohammedbinsalman")) %>%
  filter(Tipo == "Negativa") %>%
  select(Palabra) %>% 
  count(Palabra,sort = TRUE) %>% 
  top_n(100) %>% 
  wordcloud2(size = 0.5, maxRotation = 0, shuffle = FALSE)
```

## Y las palabras positivas?
Será que están relacionadas a la presentación en Teatro Colón?
```{r}
all %>% 
  filter((Palabra == "emocionado") | (Palabra == "emocionante")) %>% 
  ts_plot("30 min")
```

## Cantidad de tweets a cada 30 min. horas acuerdo a lengua
```{r, eval=FALSE}
all %>% 
  dplyr::group_by(lang) %>% 
  filter(lang %in% c("es", "en"), Dia %in% c(1, 30)) %>% 
  ts_plot( "30 min") +
  ggplot2::theme_minimal()
  #ggsave(filename = "./graficos/cantidadTiempo", device = "png")
```

#### quienes son sucks?
```{r, eval=FALSE}
all %>%
  filter(Palabra == "sucks") %>% 
    group_by(MentionsMerged) %>% 
  count(sort = TRUE)
```

#### Por que aparece asesinato?
```{r, eval=FALSE}
all %>%
  filter(Palabra == "asesinato" | Palabra %in% c("assassination", "murder", "murdering")) %>% group_by(MentionsMerged) %>% 
  count(sort = TRUE)
```
### Analisis por el tipo de sentimiento  
```{r, eval=FALSE}
all %>%
  filter(lang %in% c("es", "en")) %>% 
  group_by(Tipo, lang) %>%
  count(Palabra, sort = T) %>%
  arrange(desc(n)) %>% slice(1:3) %>%
  ggplot( aes(reorder(Palabra, n), n, fill = Tipo)) +
  geom_col() +
  facet_grid( Tipo~lang, scales = "free") +
  coord_flip() +
  tema_graf
#ggsave(filename = "./graficos/CantidadPosNeg", device = "png")
```
```{r, eval = FALSE}
tw.es %>%
  filter(lang %in% c("es", "en")) %>% 
  group_by(Tipo, lang) %>%
  count(Palabra, sort = T) %>%
  arrange(desc(n)) %>% slice(1:3) %>%
  ggplot( aes(reorder(Palabra, n), n, fill = Tipo)) +
  geom_col() +
  facet_grid( Tipo~lang, scales = "free") +
  coord_flip() +
  tema_graf
#ggsave(filename = "./graficos/CantidadPosNeg", device = "png")
```

```{r, eval=FALSE}
tw.en %>%
  filter(lang %in% c("es", "en")) %>% 
  group_by(Tipo, lang) %>%
  count(Palabra, sort = T) %>%
  arrange(desc(n)) %>% slice(1:3) %>%
  ggplot( aes(reorder(Palabra, n), n, fill = Tipo)) +
  geom_col() +
  facet_grid( Tipo~lang, scales = "free") +
  coord_flip() +
  tema_graf
#ggsave(filename = "./graficos/CantidadPosNeg", device = "png")
```

#### Sentimiento por tiempo  
```{r, eval=FALSE}
puntaje_hora <-
  tw.es %>%
  group_by(status_id) %>% # para analisar promedio por msg
  mutate(Promedio = mean(Puntuacion)) %>%
  group_by(Dia, Hora) %>%
  summarise(Media = mean(Puntuacion))

puntaje_hora %>% 
  ggplot() +
  aes(Hora, Media) +
  geom_smooth() +
  facet_wrap(~Dia) + 
  theme(legend.position = "top")
# ggsave(filename = "./graficos/tendenciasxdia", device = "png")
```
  

#### Vision general de los sentimientos por mensajes/dispositivo  
```{r, eval=FALSE}
top_source <- tw.en %>%
  group_by(source) %>%
  count() %>% 
  arrange(desc(n)) %>% 
  filter(n >= 13591)

tw.en %>% 
  right_join(top_source, by ="source") %>% 
  group_by(source) %>%
  summarise(Promedio = mean(Puntuacion)) %>%
  mutate(Tipo = ifelse(Promedio >= 0, "Positiva", "negativa")) %>% 
  ggplot(aes(source, Promedio, fill = Tipo)) + 
  geom_col()
#ggsave(filename = "./graficos/SentimientoSource", device = "png")
```

-----------------------------

```{r esta-malI, eval=FALSE}
# No anduvo! 
text <- as.data.frame(tweets$text)
text <- sub("http://([[:alnum:]|[:punct:]])+", '', text)
corpus = tm::Corpus(tm::VectorSource(text)) 
 
# Cleaning up 
# Handling UTF-8 encoding problem from the dataset 
#corpus.cleaned <- tm::tm_map(corpus, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))  
corpus.cleaned <- tm::tm_map(corpus, tm::removeWords, tm::stopwords('english')) # Removing stop-words 
corpus.cleaned <- tm::tm_map(corpus, tm::stemDocument, language = "english") # Stemming the words  
corpus.cleaned <- tm::tm_map(corpus.cleaned, tm::stripWhitespace) # Trimming excessive whitespaces

head(corpus.cleaned)
tdm <- tm::DocumentTermMatrix(corpus.cleaned) 
tdm.tfidf <- tm::weightTfIdf(tdm)
tdm.tfidf <- tm::removeSparseTerms(tdm.tfidf, 0.999) 
tfidf.matrix <- as.matrix(tdm.tfidf) 
# Cosine distance matrix (useful for specific clustering algorithms) 
dist.matrix = proxy::dist(tfidf.matrix, method = "cosine")

#clustering.kmeans <- kmeans(tfidf.matrix, truth.K) 
clustering.hierarchical <- hclust(dist.matrix, method = "ward.D2") 
clustering.dbscan <- dbscan::hdbscan(dist.matrix, minPts = 10)
```

```{r esta_malII, eval=FALSE}
# esta mal!
tweets %>%
  mutate(Dia = day(created_at),
         statusesCount_pDay = statuses_count / length(Dia)) %>%
  select(screen_name, favourites_count, statusesCount_pDay) %>%
  distinct(screen_name, favourites_count) %>% 
  arrange(desc(favourites_count)) %>%
  top_n(10)
```