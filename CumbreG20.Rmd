---
title: "BlogPost"
author: "Felipe Sodré Mendes Barros"
output:
  word_document: default
  html_document: default
---
### Antes de empezar:
Algunas referencias:  

- https://rpubs.com/jboscomendoza/analisis_sentimientos_lexico_afinn  
- https://shiring.github.io/text_analysis/2017/06/28/twitter_post   
- https://uc-r.github.io/hc_clustering   
- https://analyzecore.com/tag/twitter-sentiment-analysis/  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
# creando un tema ggplot
tema_graf <-
  theme_minimal() +
  theme(text = element_text(family = "serif"),
        panel.grid.minor = element_blank(),
        strip.background = element_rect(fill = "#EBEBEB", colour = NA),
        legend.position = "none",
        legend.box.background = element_rect(fill = "#EBEBEB", colour = NA))
```

### CumbreG20


```{r, eval=FALSE}
library(rtweet)
library(tidyverse)
library(lubridate)
# text mining library
library(tidytext)
library(wordcloud2)
library(data.table)

consumer_key <- "H4ljPlljHfQacfDhbDXCpCd37"
consumer_secret <- "CferzYTaf822R6U8lSWt995Ek681kXBnX5Gt6ysWF45TzLrYMX"
access_token <- "3323332149-x8GNuo7ziFaRgJiPnBOOH2mRLdUJuZy4Wnbe2bs"
access_secret <- "6SRUNmVoyK5cGkgYVOjblcAEFL7TyzNe1TgjaL3fy8uN3"
create_token(
  app = "ciencia de datos con R",
  consumer_key,
  consumer_secret,
  access_token,
  access_secret)



## search for 18000 tweets using the rstats hashtag
CumbreG20 <- search_tweets( "#CumbreG20", n = 18000, include_rts = FALSE, type = "recent", retryonratelimit = TRUE)
#write_as_csv(CumbreG20, "./datos/CumbreG20_7.csv")

G20Argentina <- search_tweets( "#G20Argentina", n = 18000, include_rts = FALSE, type = "recent", retryonratelimit = TRUE)
#write_as_csv(G20Argentina, "./datos/G20Argentina_7.csv")

G20Summit <- search_tweets( "#G20Summit", n = 18000, include_rts = FALSE, type = "recent", retryonratelimit = TRUE)
#write_as_csv(G20Summit, "./datos/G20Summit_7.csv")

g20org <- search_tweets( "@g20org", n = 18000, include_rts = FALSE, type = "recent", retryonratelimit = TRUE)
#write_as_csv(G20Summit, "./datos/g20org_7.csv")
```
```{r, eval=FALSE}
# Entendiendo los datos
datosList <- read_twitter_csv(list.files("./datos", full.names = TRUE)[1], unflatten = TRUE)

colnames(datosList)

datosList[1,"account_lang"]
"account_lang"
datosList[1,"verified"]
"verified"
datosList[1,"favourites_count"]
"favourites_count" 
datosList[1,"geo_coords"]
"geo_coords"
datosList[1,"quoted_location"]
datosList[1,"place_name"]

datosList[1,"quoted_screen_name"]

datosList[1,"lang"][[1]]
"lang" 
datosList[1,"mentions_screen_name"][[1]]
"mentions_screen_name"
datosList[1,"hashtags"][[1]]
"hashtags"
datosList[1,"text"]
datosList[1,"screen_name"]
datosList[1,"mentions_screen_name"][[1]]


# Necesitamos buscar una forma de leer todos los csv sin hacer falta hacelo a cada uno

datosList <- list.files("./datos", full.names = TRUE)
tweetts <- factor()
for (a in datosList){
  # a = datosList[2]
  tweet <- read_twitter_csv(a, unflatten = TRUE)
  tweets <- bind_rows(tweets, tweet)
}
# write_as_csv(tweets, "./datos/TodosDatosG20.csv")
tweets <- read_twitter_csv("./datos/TodosDatosG20.csv")
```
  
### Preguntas:
Por ahora se me ocurrió algunas preguntas como:  
  
- Cuales nombres fueron mencionados en conjunto con otros (nombre asociados entre si);  
- Que "presidente" es más influyente?
  - Más mencionado?
  - Más favoritado?
  - Sentimentos relacionados a los distintos presidentes;

### Data cleaning/organizing
```{r}
# cargando lexico
afinn <- read.csv("lexico_afinn.en.es.csv", stringsAsFactors = F, fileEncoding = "latin1") %>% 
  tbl_df()

# Identificando stopword
stopword.es <- stopwordslangs %>% filter(lang == "es" & p >= 0.80) %>% select(word)
# inglés
stopword.en <- stopwordslangs %>% filter(lang == "en" & p >= 0.80) %>% select(word)

# Separando ES, EN y filtrando dias de cumbre
tw.en <- 
  tweets %>%
  mutate(Dia = day(created_at),
         Mes = month(created_at),
         Hora = hour(created_at),
         text = tolower(text)) %>% 
  filter((Dia == 30 | Dia == 1) & lang == "en") %>%   # filtrando días de la cumbre
    unnest_tokens(input = "text", output = "Word") %>% # separa cada palabra del texto
  inner_join(afinn, ., by = "Word") %>% # join con lexico es
  mutate(Tipo = ifelse(Puntuacion > 0, "Positiva", "Negativa")) %>% 
  filter(! Palabra %in% stopword.en$word )

tw.es <- 
  tweets %>%
  mutate(Dia = day(created_at),
         Mes = month(created_at),
         Hora = hour(created_at),
         text = tolower(text)) %>% 
  filter((Dia == 30 | Dia == 1) & lang == "es") %>%   # filtrando días de la cumbre
  unnest_tokens(input = "text", output = "Palabra") %>% # separa cada palabra del texto
  inner_join(afinn, ., by = "Palabra") %>% # join con lexico es
  mutate(Tipo = ifelse(Puntuacion > 0, "Positiva", "Negativa")) %>% filter(! Palabra %in% stopword.es$word )
```

# serái interesante si pudíeramos hacer los mapas abajo en una diferenciando por el color....
```{r, eval=FALSE}
ts_plot(tw.es, "2 hours") +
  ggplot2::theme_minimal()
  #ggsave(filename = "./graficos/cantidadTiempo", device = "png")
```
```{r, eval=FALSE}
ts_plot(tw.en, "2 hours") +
  ggplot2::theme_minimal()
  #ggsave(filename = "./graficos/cantidadTiempo", device = "png")
```

### WordCloud  

```{r, eval=FALSE}
tw.en %>%
  group_by(Palabra) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  filter(n >= 250) %>% 
  wordcloud2()
```

```{r, eval=FALSE}
tw.es %>%
  group_by(Palabra) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  filter(n >= 50) %>% 
  wordcloud2()
```  

### Analisis por el tipo de sentimiento  
```{r}
tw.es %>%
  group_by(Tipo) %>%
  count(Palabra, sort = T) %>%
  arrange(desc(n)) %>% slice(1:10) %>%
  ggplot() +
  aes(Palabra, n, fill = Tipo) +
  geom_col() +
  facet_wrap( ~Tipo, scales = "free") +
  coord_flip()
#ggsave(filename = "./graficos/CantidadPosNeg", device = "png")
```
  
```{r}
tw.en %>%
  group_by(Tipo) %>%
  count(Palabra, sort = T) %>%
  arrange(desc(n)) %>% slice(1:10) %>%
  ggplot() +
  aes(Palabra, n, fill = Tipo) +
  geom_col() +
  facet_wrap( ~Tipo, scales = "free") +
  coord_flip()
#ggsave(filename = "./graficos/CantidadPosNeg", device = "png")
```  
```{r}
puntaje_hora <-
  tw.es %>%
  group_by(status_id) %>% # para analisar promedio por msg
  mutate(Promedio = mean(Puntuacion)) %>%
  group_by(Dia, Hora) %>%
  summarise(Media = mean(Puntuacion))

puntaje_hora %>% 
  ggplot() +
  aes(Hora, Media) +
  geom_smooth() +
  facet_wrap(~Dia) + 
  theme(legend.position = "top")
# ggsave(filename = "./graficos/tendenciasxdia", device = "png")
```
  
```{r}
puntaje_hora.en <-
  tw.en %>%
  group_by(status_id) %>% # para analisar promedio por msg
  mutate(Promedio = mean(Puntuacion)) %>%
  group_by(Dia, Hora) %>%
  summarise(Media = mean(Puntuacion))

puntaje_hora.en %>% 
  ggplot() +
  aes(Hora, Media) +
  geom_smooth() +
  facet_wrap(~Dia) + 
  theme(legend.position = "top")
# ggsave(filename = "./graficos/tendenciasxdia", device = "png")
```



#### Vision general de los sentimientos por mensajes 
```{r}
top_source <- tw.en %>%
  group_by(source) %>%
  count() %>% 
  arrange(desc(n)) %>% 
  filter(n >= 13591)

tw.en %>% 
  right_join(top_source, by ="source") %>% 
  group_by(source) %>%
  summarise(Promedio = mean(Puntuacion)) %>%
  mutate(Tipo = ifelse(Promedio >= 0, "Positiva", "negativa")) %>% 
  ggplot(aes(source, Promedio, fill = Tipo)) + 
  geom_col()
#ggsave(filename = "./graficos/SentimientoSource", device = "png")
```

```{r}
top_source <- tw.es %>%
  group_by(source) %>%
  count() %>% 
  arrange(desc(n)) %>% 
  filter(n >= 1292)

tw.en %>% 
  right_join(top_source, by ="source") %>% 
  group_by(source) %>%
  summarise(Promedio = mean(Puntuacion)) %>%
  mutate(Tipo = ifelse(Promedio >= 0, "Positiva", "negativa")) %>% 
  ggplot(aes(source, Promedio, fill = Tipo)) + 
  geom_col()
#ggsave(filename = "./graficos/SentimientoSource", device = "png")
```
